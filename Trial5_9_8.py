{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "from scipy.spatial.distance import squareform\n",
    "import time \n",
    "import re\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# -------------- Data acquisition part\n",
    "\n",
    "def getImgpaths(folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the location of all the images in the similarity\n",
    "    matrix order.\n",
    "\n",
    "    Parameters:\n",
    "        folder: Location of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    locs = {}  # Dictionary with the locations\n",
    "    unord_locs = {}\n",
    "    # Gets all the images knowing all directories have the same structure.\n",
    "    lvl_cam = {}  # Stores the amount of images in each cam folder\n",
    "    n = 0\n",
    "    for heir in sorted(os.walk(folder)):\n",
    "        if heir[1] == []:  # if you are in the last folder of a tree\n",
    "            lvl_cam[heir[0]] = len(heir[2])  # save the number of images\n",
    "            for img in sorted(heir[2]):\n",
    "                loc = heir[0] + '/' + img  # add to the path the img\n",
    "                unord_locs[n] = loc  # and store the path\n",
    "                n += 1\n",
    "\n",
    "    # Order them in the similarity matrix order\n",
    "    N = max(lvl_cam.values())\n",
    "    nn = 0\n",
    "    seq, cam = searchSeq(unord_locs)\n",
    "    aa = '-'.join(list(unord_locs.values()))\n",
    "    for s in seq:\n",
    "        for n in range(2*N):\n",
    "            n = str(n)\n",
    "            i = '0'*(3-len(n)) + n + '.png'\n",
    "            for c in cam:\n",
    "                name = folder + s + c + '/' + i\n",
    "                sear = re.search(name, aa)\n",
    "                if sear is not None:  # check if is in the list\n",
    "                    locs[nn] = name\n",
    "                    nn += 1\n",
    "\n",
    "    return locs\n",
    "\n",
    "\n",
    "def searchSeq(my_dict):\n",
    "    \"\"\"\n",
    "    Searches for the different levels inside the \n",
    "    folder tree.\n",
    "\n",
    "    mydict: is the dictionary with the path of all the\n",
    "        files in the folder tree.\n",
    "    \"\"\"\n",
    "    lvl = []\n",
    "    seq = []  # list of sequence folders\n",
    "    cam = []  # list of cam folders\n",
    "    pos = []\n",
    "    values = my_dict.values()\n",
    "    for name in values:\n",
    "        for match in re.finditer('/', name):\n",
    "            p = match.start()\n",
    "            pos.append(p)  # find all positions of /\n",
    "        for n in range(len(pos)-1):\n",
    "            start = pos[n]\n",
    "            end = pos[n+1]\n",
    "            lvl.append(name[start:end])\n",
    "        pos = []\n",
    "\n",
    "    lvl = list(dict.fromkeys(lvl))\n",
    "    for l in lvl:\n",
    "        if re.search(\"Sequence\", l):\n",
    "            seq.append(l)\n",
    "        if re.search(\"cam\", l):\n",
    "            cam.append(l)\n",
    "\n",
    "    return seq, cam\n",
    "\n",
    "\n",
    "def get_grps(folder):\n",
    "    \"\"\"\n",
    "    Function 2: Input la similarity matrix y devuelve un diccionario que clasifica \n",
    "    en grupos las fotos que son parecidas segÃºn el input.\n",
    "    \"\"\"\n",
    "\n",
    "    ini = folder.find('dataset/')\n",
    "    name = folder[ini + 8: ini + 8 + 3]\n",
    "\n",
    "    simil_path = folder + \"/\" + name + \"_similarity.h5\"\n",
    "    with h5py.File(simil_path, \"r\") as f:\n",
    "        # similarity labels in condensed form (shape=(1,n * (n-1) / 2))\n",
    "        gt_labels = f[\"sim\"][:].flatten()\n",
    "        # similarity labels in matrix form (shape=(n, n))\n",
    "        gt_labels = squareform(gt_labels)\n",
    "\n",
    "    aa = []\n",
    "    a = np.triu(gt_labels)\n",
    "    b = a  # np.zeros(np.shape(a)) duplicate variable\n",
    "    test_dict = {}  # create a test dictionary\n",
    "    # test_dict = {'grp': 'matches'}  # create a test dictionary\n",
    "    grp_count = 0\n",
    "    aa = np.zeros(np.shape(a))  # initializes the mask creates a zero array\n",
    "\n",
    "    for z in range(0, len(a)):  # number of rows increasing\n",
    "        # gets the matches in each group, in row one gets matched column for each row\n",
    "        temp = np.nonzero(b[z, :])\n",
    "        # im setting all row values in b to false, temp is particular row\n",
    "        b[temp, :] = False\n",
    "        match_list = []\n",
    "\n",
    "        if np.size(np.nonzero(temp)) > 0:  # it has a match\n",
    "            #print(np.nonzero(temp))\n",
    "            grp_count = grp_count+1\n",
    "\n",
    "            for t in temp:\n",
    "                # print(z,t)\n",
    "                t = np.append(z, t)  # has all the values\n",
    "                match_list.append(t)\n",
    "\n",
    "            aa[grp_count][0:len(np.asarray(match_list)[0])] = np.asarray(\n",
    "                match_list)  # converting dictionary into an array\n",
    "\n",
    "    # you look into the array and get the index numbers of non duplicated values\n",
    "    vals, ind = np.unique(aa, return_index=True)\n",
    "    # another mask with all zeros again, same size as aa\n",
    "    bb = np.zeros(np.shape(aa.flatten()))\n",
    "    bb[ind] = 1  # bb of all unique values are iqual to 1\n",
    "    bb = np.reshape(bb, np.shape(aa))  # shaping bb as the same shape as aa\n",
    "    # multiplying the masks, all the unique values will be written\n",
    "    cc = (bb*aa).astype(int)\n",
    "    grp_count = 0  # the rows of the array cc are groups, the columns are the index values of all matched images in a group\n",
    "    final_dict = {}\n",
    "    # final_dict = {'grp': 'indices'}\n",
    "    for n in range(0, len(a)-1):\n",
    "        # gets all the index values which are matched for a particular row\n",
    "        temp1 = np.nonzero(cc[n])\n",
    "        if np.size(temp1) > 0:  # if there is a match\n",
    "            # increase group count and add it to one group of the final dictionary\n",
    "            if n == 1:\n",
    "                temp1 = np.insert(temp1, 0, [0])\n",
    "            final_dict['grp%d' % grp_count] = cc[n][temp1]\n",
    "            grp_count = grp_count+1\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def setGroups(loc_dict: dict, grp_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the location of all the images by groups\n",
    "    set by the similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "        loc_dict: Dictionary with the locations of the images in the\n",
    "            similarity matrix order.\n",
    "        grp_dict: Dictionary with the group information of the similarity\n",
    "            matrix specifying the indexes of the images in each group.\n",
    "    \"\"\"\n",
    "    loc_bygrp = {}\n",
    "\n",
    "    for grp in grp_dict.keys():\n",
    "        indexes = grp_dict[grp]\n",
    "        loc_bygrp[grp] = []\n",
    "        for ind in indexes:\n",
    "            loc_bygrp[grp].append(loc_dict[ind])\n",
    "        loc_bygrp[grp].sort()\n",
    "\n",
    "    return loc_bygrp\n",
    "\n",
    "# -------------- Data acquisition part\n",
    "\n",
    "def get_images(folder) -> dict:\n",
    "    \"\"\"\n",
    "    Get images from a folder and put them into dictionaries.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    if folder == \"ddataset/train\":\n",
    "        img_list = []\n",
    "        dic = {\"a\": [\"a01.png\", \"a02.png\",\n",
    "                     \"a03.png\", \"a04.png\", \"a05.png\"],\n",
    "               \"b\": [\"b02.png\"],\n",
    "               \"c\": [\"c01.png\"]}\n",
    "        for k in dic.keys():\n",
    "            for filename in dic[k]:\n",
    "                path = folder + \"/\" + filename\n",
    "                img = cv.imread(path, 0)  # Read in grayscale\n",
    "                img_list.append(img)\n",
    "            images[k] = img_list\n",
    "            img_list = []\n",
    "    else:\n",
    "        for filename in os.listdir(folder):\n",
    "            category = filename  # Dictionary name is each image name\n",
    "            path = folder + \"/\" + filename\n",
    "            img = cv.imread(path, 0)  # Read in grayscale\n",
    "            images[category] = img\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class orb_features:\n",
    "    \"\"\"\n",
    "    ORB detector\n",
    "    \n",
    "    Attributes:\n",
    "        param: Parameters of the ORB detector.\n",
    "        detector: Detector object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.param = dict(nfeatures=50, scaleFactor=1.2, nlevels=4,\n",
    "                          edgeThreshold=31, firstLevel=0, WTA_K=2,\n",
    "                          scoreType=cv.ORB_HARRIS_SCORE,\n",
    "                          patchSize=31, fastThreshold=20)\n",
    "        self.detector = cv.ORB_create(**self.param)\n",
    "\n",
    "    def detectFromDict(self, images_dict: dict) -> [list, dict]:\n",
    "        \"\"\"\n",
    "        Get ORB features from a dictionary of images and save\n",
    "        them in a new dictionary.\n",
    "        \"\"\"\n",
    "        orb = self.detector\n",
    "        descriptor_list = []  # a list of all features\n",
    "        l = [] # a list for the features of each category\n",
    "        descriptor_bycat = {}  # a dictionary of features by categories\n",
    "\n",
    "        for labels, img_loc in images_dict.items():\n",
    "            print(type(img_loc))\n",
    "            if isinstance(img_loc, list):\n",
    "                # if is a list then it is the dictionary with the classified images\n",
    "                for n in range(len(img_loc)):\n",
    "                    nimg_loc = img_loc[n]\n",
    "                    nimg = cv.imread(nimg_loc, 0)\n",
    "                    kp, des = orb.detectAndCompute(nimg, None)\n",
    "                    descriptor_list.extend(des)\n",
    "                    l.extend(des)\n",
    "                descriptor_bycat[labels] = l\n",
    "                l = []\n",
    "            else: \n",
    "                img = cv.imread(img_loc, 0)\n",
    "                kp, des = orb.detectAndCompute(img, None)\n",
    "                descriptor_list.extend(des)\n",
    "                descriptor_bycat[labels] = des\n",
    "\n",
    "        return descriptor_list, descriptor_bycat\n",
    "\n",
    "    def normalizeAllFeatures(self, trainbycat: dict, testbycat: dict): \n",
    "        allfeatureslist = []\n",
    "        traininfo = {}\n",
    "        testinfo = {}\n",
    "        trainlist = []\n",
    "        testlist = []\n",
    "        trainbycat_n = {}\n",
    "        testbycat_n = {}\n",
    "\n",
    "        # Get a list of all features\n",
    "        for cat, features in trainbycat.items():\n",
    "            traininfo[cat] = len(features)\n",
    "            allfeatureslist.extend(features)\n",
    "        for cat, features in testbycat.items():\n",
    "            testinfo[cat] = len(features)\n",
    "            allfeatureslist.extend(features)\n",
    "\n",
    "        # Normalize it\n",
    "        allfeatureslist = preprocessing.normalize(allfeatureslist, norm='l2')\n",
    "\n",
    "        # Get the lists of normalized values for train and test\n",
    "        ntrain = sum(traininfo.values()) \n",
    "        ntest = sum(testinfo.values())\n",
    "        trainlist = allfeatureslist[0:ntrain-1]\n",
    "        testlist = allfeatureslist[ntrain:ntrain+ntest-1]\n",
    "\n",
    "        # Organize normalized features by class for train and test\n",
    "        for cat, nfeatures in traininfo.items():\n",
    "            trainbycat_n[cat] = allfeatureslist[0:nfeatures]\n",
    "            allfeatureslist = allfeatureslist[nfeatures:]\n",
    "        for cat, nfeatures in testinfo.items():\n",
    "            testbycat_n[cat] = allfeatureslist[0:nfeatures]\n",
    "            allfeatureslist = allfeatureslist[nfeatures:]\n",
    "\n",
    "        return trainlist, trainbycat_n, testlist, testbycat_n\n",
    "\n",
    "\n",
    "class bow():\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Attributes:\n",
    "            t: numerical threshold for classifying.\n",
    "            vw_centers: list of the cluster centers for the visual words.\n",
    "        \"\"\"\n",
    "        self.t = 1 \n",
    "        self.vw_centers = []\n",
    "        self.nclusters = 150\n",
    "    \n",
    "    def setThreshold(self, threshold):\n",
    "        \"\"\"\n",
    "        Sets a new classifying threshold.\n",
    "        \"\"\"\n",
    "        self.t = threshold\n",
    "        \n",
    "    def kmeans(self, descriptor_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A k-means clustering algorithm.\n",
    "        \n",
    "        Inputs:\n",
    "            descriptor_list: Descriptors list\n",
    "                (unordered nfeatures x 32 matrix).\n",
    "\n",
    "        Returns: A matrix [nclusters, 32] that holds \n",
    "            central points of the clusters.\n",
    "        \"\"\"\n",
    "        cluster = KMeans(n_clusters=self.nclusters, n_init=10)\n",
    "        cluster.fit(descriptor_list)\n",
    "        self.vw_centers = cluster.cluster_centers_ \n",
    "\n",
    "        return cluster.cluster_centers_\n",
    "\n",
    "    def compute_histogram(self, featuresbycat: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the histogram for every category.\n",
    "\n",
    "        Inputs:\n",
    "            featuresbycat: Dictionary that contains features \n",
    "                of each category.\n",
    "            centers: A matrix [nclusters, 32] that holds \n",
    "                central points of the clusters.\n",
    "\n",
    "        Returns: A dictionary with the histograms of\n",
    "            each category.\n",
    "        \"\"\"\n",
    "        histogrambycat = {}  # dictionary of histograms\n",
    "        centers = self.vw_centers\n",
    "        histogram = np.zeros([1, self.nclusters])\n",
    "        for cat, features in featuresbycat.items():\n",
    "            for n in range(len(features)):\n",
    "                word = self.matchWord(features[n])\n",
    "                histogram[0, word] += 1\n",
    "            m = np.amax(histogram)\n",
    "            histogram = histogram/m\n",
    "            histogrambycat[cat] = histogram\n",
    "            histogram = np.zeros([1, self.nclusters])\n",
    "\n",
    "        return histogrambycat\n",
    "\n",
    "    def matchWord(self, feature) -> int:\n",
    "        \"\"\"\n",
    "        Finds the matching word searching for the minimum \n",
    "        euclidean distance.\n",
    "\n",
    "        Inputs:\n",
    "            feature: A single feature (descriptor).\n",
    "            centers: A matrix [nclusters, 32] that holds central \n",
    "                points of the clusters.\n",
    "        Returns: \n",
    "            Index of the matched word.\n",
    "        \"\"\"\n",
    "        centers = self.vw_centers\n",
    "        # distance.cdist(coords, coords, 'euclidean')\n",
    "        l = len(centers)\n",
    "        f = np.tile(feature, l)\n",
    "        c = np.reshape(centers,(1,-1))\n",
    "        d = np.linalg.norm(np.reshape(f-c, (l, -1)), axis=1)\n",
    "        minind = np.argmin(d)\n",
    "        # for n in range(centers.shape[0]):\n",
    "        #     dist = np.linalg.norm(feature - centers[n, :])  # L2-norm\n",
    "        #     if n == 0:\n",
    "        #         mindist = dist\n",
    "        #         minind = n\n",
    "        #     elif dist < mindist:\n",
    "        #         mindist = dist\n",
    "        #         minind = n\n",
    "\n",
    "        return minind\n",
    "\n",
    "    def matchCategory(self, trainh: dict, testh: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Finds the matching category for every image in test dataset using 1NN\n",
    "        Nearest Neightbour with k=1.\n",
    "\n",
    "        Inputs:\n",
    "            trainh: Dictionary of histograms by class, from training.\n",
    "            testh: Dictionary of histograms by image, from test.\n",
    "        Returns: \n",
    "            Dictionary of the test images classified.\n",
    "        \"\"\"\n",
    "        img_classified = {}\n",
    "        classification = np.nan\n",
    "\n",
    "        flat_te = []\n",
    "        for sublist in list(testh.values()):\n",
    "            for item in sublist:\n",
    "                flat_te.append(item)\n",
    "        flat_tr = []\n",
    "        for sublist in list(trainh.values()):\n",
    "            for item in sublist:\n",
    "                flat_tr.append(item)\n",
    "        d = distance.cdist(flat_te, flat_tr, metric='euclidean')\n",
    "        mask = np.empty(d.shape)\n",
    "        mask[:] = np.inf\n",
    "        mask[d < np.mean(d)] = 1\n",
    "        d_masked = np.multiply(d,mask)\n",
    "        class_mask = np.sum(~np.isinf(mask),axis=1)==0 # check if all elements in a row are inf\n",
    "        classification = np.argmin(d_masked, axis=1).astype(float)\n",
    "        classification[class_mask] = np.nan\n",
    "        keys = list(testh.keys())\n",
    "        img_classified = dict(zip(keys, classification.T))\n",
    "        \n",
    "        # distance.cdist(coords, coords, 'euclidean')\n",
    "        #  aqui habria que calcular la distancia y meter\n",
    "        #  el threshold de algun modo\n",
    "        # for imgname, histtest in testh.items():\n",
    "        #     for cat, hist in trainh.items():\n",
    "        #         dist = np.linalg.norm(histtest - hist)  # L2-norm\n",
    "        #         if dist < self.t:\n",
    "        #             classification = cat\n",
    "        #     img_classified[imgname] = classification\n",
    "        #     classification = np.nan\n",
    "        \n",
    "        similMat = self.createSimilarityMat(img_classified)\n",
    "\n",
    "        return similMat\n",
    "    \n",
    "    def createSimilarityMat(self, classifications: dict):\n",
    "        img_names = list(classifications.keys())\n",
    "        img_cats = list(classifications.values())\n",
    "        size = len(img_names)\n",
    "        similarityMat = np.zeros([size, size])\n",
    "        for ii in range(size):\n",
    "            img_name = img_names[ii]\n",
    "            cat = classifications[img_name]\n",
    "            for jj in range(size):\n",
    "                c = img_cats[jj]\n",
    "                if cat == c and ii != jj:\n",
    "                    similarityMat[ii,jj] = 1\n",
    "        return similarityMat\n",
    "\n",
    "    def findThreshold(self, trainh: dict, testh: dict):\n",
    "        \"\"\"\n",
    "        Finds ans sets a new threshold based on the distances \n",
    "        of the test and training histogram sets.\n",
    "        \"\"\"\n",
    "        flat_te = []\n",
    "        for sublist in list(testh.values()):\n",
    "            for item in sublist:\n",
    "                flat_te.append(item)\n",
    "        flat_tr = []\n",
    "        for sublist in list(trainh.values()):\n",
    "            for item in sublist:\n",
    "                flat_tr.append(item)\n",
    "        d = distance.cdist(flat_te, flat_tr, metric='euclidean')\n",
    "        distmin = np.min(d)\n",
    "        distmean = np.mean(d)\n",
    "        t = distmean - (distmean-distmin)/2\n",
    "        \n",
    "        # dist_list = np.array([])\n",
    "        # for imgname, histtest in testh.items():\n",
    "        #     for cat, hist in trainh.items():\n",
    "        #         dist = np.linalg.norm(histtest - hist)  # L2-norm\n",
    "        #         dist_list = np.append(dist_list, dist)\n",
    "        # distmin = np.min(dist_list)\n",
    "        # distmean = np.mean(dist_list)\n",
    "        # t = distmean - (distmean-distmin)/2\n",
    "        self.setThreshold(t)\n",
    "\n",
    "\n",
    "def save_obj(name, obj):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def main(t):\n",
    "    start_time = time.time()\n",
    "    train_folder = \"dataset/W17\"  # Location of the dataset.\n",
    "    test_folder = \"dataset/W17\"  \n",
    "\n",
    "    pathtr = (\"features/%s_tr.pkl\" % (train_folder[-3:]))\n",
    "    pathte = (\"features/%s_te.pkl\" % (test_folder[-3:]))\n",
    "\n",
    "    if not os.path.exists('features'):\n",
    "        os.makedirs('features')  # create a features dir\n",
    "\n",
    "    if os.path.exists(pathtr) and os.path.exists(pathte):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"--- Obtaining images: %s seconds ---\" % (time.time() - start_time))\n",
    "        # Get the training dataset\n",
    "        train_dict = getImgpaths(train_folder)\n",
    "        grp_dict = get_grps(train_folder)\n",
    "        loc_bygrp = setGroups(train_dict, grp_dict)\n",
    "        # Get the test dataset\n",
    "        test_dict = getImgpaths(train_folder)\n",
    "        print(\"--- Images obtained: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # # Obtain images by name and by class from dataset\n",
    "    # train_dset_bycat = get_images(\"ddataset/train\")\n",
    "    # # train_dset_byname = get_images(\"ddataset/train\")\n",
    "    # test_dset_byname = get_images(\"ddataset/test\")  # take test images\n",
    "\n",
    "    # Gets all the ORB features \n",
    "    print(\"--- Obtaining features: %s seconds ---\" % (time.time() - start_time))\n",
    "    orb = orb_features()\n",
    "    if os.path.exists(pathtr) and os.path.exists(pathte):\n",
    "        print(\"--- Loading.. ---\")\n",
    "        train_featuresbycat = load_obj(pathtr)\n",
    "        test_featuresbycat = load_obj(pathte)\n",
    "    else:\n",
    "        print(\"--- Calculating.. ---\")\n",
    "        # Gets all the features from the training images\n",
    "        train_features, train_featuresbycat = orb.detectFromDict(loc_bygrp)\n",
    "        save_obj(pathtr, train_featuresbycat)  # save it for later uses\n",
    "        # Get all the features from the test images\n",
    "        test_features, test_featuresbycat = orb.detectFromDict(test_dict)\n",
    "        save_obj(pathte, test_featuresbycat)  # save it for later uses\n",
    "    print(\"--- Features ontained: %s seconds ---\" % (time.time() - start_time))\n",
    "    # Normalize them\n",
    "    train_features, train_featuresbycat, test_features, test_featuresbycat = \\\n",
    "    orb.normalizeAllFeatures(train_featuresbycat, test_featuresbycat)\n",
    "\n",
    "    # Initialize BOW\n",
    "    # bow_obj = load_obj(\"features/bowc.pkl\")\n",
    "    print(\"--- Beginning BOW: %s seconds ---\" % (time.time() - start_time))\n",
    "    bow_obj = bow()\n",
    "    # Clusters the features using kmeans\n",
    "    print(\"--- Computing centers: %s seconds ---\" % (time.time() - start_time))\n",
    "    bow_obj.kmeans(train_features)\n",
    "    print(\"--- Centers computed: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Compute histograms for every category in train dataset\n",
    "    print(\"--- Computing histograms for train: %s seconds ---\" % (time.time() - start_time))\n",
    "    train_histogrambycat = bow_obj.compute_histogram(train_featuresbycat)\n",
    "    # Compute histrograms for every image in test dataset\n",
    "    print(\"--- Computing histograms for test: %s seconds ---\" % (time.time() - start_time))\n",
    "    test_histogrambycat = bow_obj.compute_histogram(test_featuresbycat)\n",
    "\n",
    "    # Find new threshold\n",
    "    #bow_obj.findThreshold(train_histogrambycat, test_histogrambycat)\n",
    "    bow_obj.setThreshold(t)\n",
    "    \n",
    "    # Classify the test images into the trainned categories\n",
    "    print(\"--- Classifying test images: %s seconds ---\" % (time.time() - start_time))\n",
    "    classification_results = bow_obj.matchCategory(\n",
    "        train_histogrambycat, test_histogrambycat)\n",
    "    print(\"Classification results: \")\n",
    "    print(classification_results)\n",
    "\n",
    "    # Evaluation\n",
    "    with h5py.File(\"W17_similarity.h5\", \"r\") as f: # load similarity\n",
    "        gt_labels = f[\"sim\"][:].flatten() #similarity labels in condensed form (shape=(1,n * (n-1) / 2))\n",
    "        gt_labels = squareform(gt_labels) #similarity labels in matrix form (shape=(n, n))\n",
    "    \n",
    "    [Precision, Recall, F1] = PR(gt_labels,classification_results)\n",
    "    \n",
    "    return Precision, Recall, F1 \n",
    "\n",
    "def PR(gt_labels,ts_labels):\n",
    "\n",
    "#\" Evaluate Classification \"\n",
    "# Input: Similarity matrix of classification and gound truth\n",
    "\n",
    "    [n,m] = gt_labels.shape\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range (0,n):\n",
    "        for j in range (0,m):\n",
    "            if ts_labels[i][j] == True:\n",
    "                if gt_labels[i][j] == True:\n",
    "                    TP+=1\n",
    "                elif gt_labels[i][j] == False:\n",
    "                    FP+=1\n",
    "            elif ts_labels[i][j] == False:\n",
    "                if gt_labels[i][j] == False:\n",
    "                    TN+=1\n",
    "                elif gt_labels[i][j] == True:\n",
    "                    FN+=1\n",
    "            \n",
    "            \n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1 = 2*(Precision*Recall)/(Precision+Recall)\n",
    "\n",
    "    print(Precision)\n",
    "    print(Recall)\n",
    "    print(F1)\n",
    "    \n",
    "    return Precision, Recall, F1\n",
    "\n",
    "## Evaluation \n",
    "\n",
    "def aveP(Precision, Recall):\n",
    "# Precision and Recall should be a array (by varying threshold)\n",
    "    n = len(Precision)\n",
    "    AP = 0\n",
    "    for i in range(0,n):\n",
    "        Recall = B[i]\n",
    "        Precision = A[i]\n",
    "        if i !=0:\n",
    "            AP+=(Recall-pre)*Precision\n",
    "        else:\n",
    "            AP+=(Recall-0)*Precision\n",
    "        pre = Recall\n",
    "        \n",
    "    return AP\n",
    "\n",
    "def quaternion(qi):\n",
    "# Compute delta phi from quaterions\n",
    "# Input: pose-liked quaternion\n",
    "    \n",
    "    [n,m] = qi.shape\n",
    "    phi = np.zeros((n,n))\n",
    "    for i in range (0,n):\n",
    "        for j in range (0,n):\n",
    "            InProd = 0\n",
    "            if i == j:\n",
    "                phi[i][j] = 0 ;\n",
    "            else:\n",
    "                for k in range (3,7):\n",
    "                    InProd += qi[i][k]*qi[j][k]\n",
    "                \n",
    "                distance = 1 - InProd**2\n",
    "                phi[i][j] = math.acos(-2*distance+1)*180/math.pi  \n",
    "    return phi\n",
    "\n",
    "## Main code\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    # Create empty array\n",
    "    arrayP = []\n",
    "    arrayR = []\n",
    "    arrayF = []\n",
    "    \n",
    "    # Set different classification threshold\n",
    "    for i in range(0,11):\n",
    "        [Precision, Recall, F1] = main(i)\n",
    "        arrayP.append(Precision)\n",
    "        arrayR.append(Recall)\n",
    "        arrayF.append(F1)\n",
    "    \n",
    "    # Depict PR curve\n",
    "    plt.figure(1)\n",
    "    plt.plot(arrayR,arrayP)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average precision\n",
    "    AP = aveP(arrayP, arrayR) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
