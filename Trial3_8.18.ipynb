{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "from scipy.spatial.distance import squareform\n",
    "import time \n",
    "import re\n",
    "# from scipy.spatial import distance\n",
    "\n",
    "# -------------- Data acquisition part\n",
    "\n",
    "def getImgpaths(folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the location of all the images in the similarity\n",
    "    matrix order.\n",
    "    Parameters:\n",
    "        folder: Location of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    locs = {}  # Dictionary with the locations\n",
    "    unord_locs = {}\n",
    "    # Gets all the images knowing all directories have the same structure.\n",
    "    lvl_cam = {}  # Stores the amount of images in each cam folder\n",
    "    n = 0\n",
    "    for heir in sorted(os.walk(folder)):\n",
    "        if heir[1] == []:  # if you are in the last folder of a tree\n",
    "            lvl_cam[heir[0]] = len(heir[2])  # save the number of images\n",
    "            for img in sorted(heir[2]):\n",
    "                loc = heir[0] + '/' + img  # add to the path the img\n",
    "                unord_locs[n] = loc  # and store the path\n",
    "                n += 1\n",
    "\n",
    "    # Order them in the similarity matrix order\n",
    "    N = max(lvl_cam.values())\n",
    "    nn = 0\n",
    "    seq, cam = searchSeq(unord_locs)\n",
    "    aa = '-'.join(list(unord_locs.values()))\n",
    "    for s in seq:\n",
    "        for n in range(2*N):\n",
    "            n = str(n)\n",
    "            i = '0'*(3-len(n)) + n + '.png'\n",
    "            for c in cam:\n",
    "                name = folder + s + c + '/' + i\n",
    "                sear = re.search(name, aa)\n",
    "                if sear is not None:  # check if is in the list\n",
    "                    locs[nn] = name\n",
    "                    nn += 1\n",
    "\n",
    "    return locs\n",
    "\n",
    "\n",
    "def searchSeq(my_dict):\n",
    "    \"\"\"\n",
    "    Searches for the different levels inside the \n",
    "    folder tree.\n",
    "    mydict: is the dictionary with the path of all the\n",
    "        files in the folder tree.\n",
    "    \"\"\"\n",
    "    lvl = []\n",
    "    seq = []  # list of sequence folders\n",
    "    cam = []  # list of cam folders\n",
    "    pos = []\n",
    "    values = my_dict.values()\n",
    "    for name in values:\n",
    "        for match in re.finditer('/', name):\n",
    "            p = match.start()\n",
    "            pos.append(p)  # find all positions of /\n",
    "        for n in range(len(pos)-1):\n",
    "            start = pos[n]\n",
    "            end = pos[n+1]\n",
    "            lvl.append(name[start:end])\n",
    "        pos = []\n",
    "\n",
    "    lvl = list(dict.fromkeys(lvl))\n",
    "    for l in lvl:\n",
    "        if re.search(\"Sequence\", l):\n",
    "            seq.append(l)\n",
    "        if re.search(\"cam\", l):\n",
    "            cam.append(l)\n",
    "\n",
    "    return seq, cam\n",
    "\n",
    "\n",
    "def get_grps(folder):\n",
    "    \"\"\"\n",
    "    Function 2: Input la similarity matrix y devuelve un diccionario que clasifica \n",
    "    en grupos las fotos que son parecidas segÃºn el input.\n",
    "    \"\"\"\n",
    "\n",
    "    ini = folder.find('dataset/')\n",
    "    name = folder[ini + 8: ini + 8 + 3]\n",
    "\n",
    "    simil_path = folder + \"/\" + name + \"_similarity.h5\"\n",
    "    with h5py.File(simil_path, \"r\") as f:\n",
    "        # similarity labels in condensed form (shape=(1,n * (n-1) / 2))\n",
    "        gt_labels = f[\"sim\"][:].flatten()\n",
    "        # similarity labels in matrix form (shape=(n, n))\n",
    "        gt_labels = squareform(gt_labels)\n",
    "\n",
    "    aa = []\n",
    "    a = np.triu(gt_labels)\n",
    "    b = a  # np.zeros(np.shape(a)) duplicate variable\n",
    "    test_dict = {}  # create a test dictionary\n",
    "    # test_dict = {'grp': 'matches'}  # create a test dictionary\n",
    "    grp_count = 0\n",
    "    aa = np.zeros(np.shape(a))  # initializes the mask creates a zero array\n",
    "\n",
    "    for z in range(0, len(a)):  # number of rows increasing\n",
    "        # gets the matches in each group, in row one gets matched column for each row\n",
    "        temp = np.nonzero(b[z, :])\n",
    "        # im setting all row values in b to false, temp is particular row\n",
    "        b[temp, :] = False\n",
    "        match_list = []\n",
    "\n",
    "        if np.size(np.nonzero(temp)) > 0:  # it has a match\n",
    "            #print(np.nonzero(temp))\n",
    "            grp_count = grp_count+1\n",
    "\n",
    "            for t in temp:\n",
    "                # print(z,t)\n",
    "                t = np.append(z, t)  # has all the values\n",
    "                match_list.append(t)\n",
    "\n",
    "            aa[grp_count][0:len(np.asarray(match_list)[0])] = np.asarray(\n",
    "                match_list)  # converting dictionary into an array\n",
    "\n",
    "    # you look into the array and get the index numbers of non duplicated values\n",
    "    vals, ind = np.unique(aa, return_index=True)\n",
    "    # another mask with all zeros again, same size as aa\n",
    "    bb = np.zeros(np.shape(aa.flatten()))\n",
    "    bb[ind] = 1  # bb of all unique values are iqual to 1\n",
    "    bb = np.reshape(bb, np.shape(aa))  # shaping bb as the same shape as aa\n",
    "    # multiplying the masks, all the unique values will be written\n",
    "    cc = (bb*aa).astype(int)\n",
    "    grp_count = 0  # the rows of the array cc are groups, the columns are the index values of all matched images in a group\n",
    "    final_dict = {}\n",
    "    # final_dict = {'grp': 'indices'}\n",
    "    for n in range(0, len(a)-1):\n",
    "        # gets all the index values which are matched for a particular row\n",
    "        temp1 = np.nonzero(cc[n])\n",
    "        if np.size(temp1) > 0:  # if there is a match\n",
    "            # increase group count and add it to one group of the final dictionary\n",
    "            if n == 1:\n",
    "                temp1 = np.insert(temp1, 0, [0])\n",
    "            final_dict['grp%d' % grp_count] = cc[n][temp1]\n",
    "            grp_count = grp_count+1\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def setGroups(loc_dict: dict, grp_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the location of all the images by groups\n",
    "    set by the similarity matrix.\n",
    "    Parameters:\n",
    "        loc_dict: Dictionary with the locations of the images in the\n",
    "            similarity matrix order.\n",
    "        grp_dict: Dictionary with the group information of the similarity\n",
    "            matrix specifying the indexes of the images in each group.\n",
    "    \"\"\"\n",
    "    loc_bygrp = {}\n",
    "\n",
    "    for grp in grp_dict.keys():\n",
    "        indexes = grp_dict[grp]\n",
    "        loc_bygrp[grp] = []\n",
    "        for ind in indexes:\n",
    "            loc_bygrp[grp].append(loc_dict[ind])\n",
    "        loc_bygrp[grp].sort()\n",
    "\n",
    "    return loc_bygrp\n",
    "\n",
    "# -------------- Data acquisition part\n",
    "\n",
    "def get_images(folder) -> dict:\n",
    "    \"\"\"\n",
    "    Get images from a folder and put them into dictionaries.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    if folder == \"ddataset/train\":\n",
    "        img_list = []\n",
    "        dic = {\"a\": [\"a01.png\", \"a02.png\",\n",
    "                     \"a03.png\", \"a04.png\", \"a05.png\"],\n",
    "               \"b\": [\"b02.png\"],\n",
    "               \"c\": [\"c01.png\"]}\n",
    "        for k in dic.keys():\n",
    "            for filename in dic[k]:\n",
    "                path = folder + \"/\" + filename\n",
    "                img = cv.imread(path, 0)  # Read in grayscale\n",
    "                img_list.append(img)\n",
    "            images[k] = img_list\n",
    "            img_list = []\n",
    "    else:\n",
    "        for filename in os.listdir(folder):\n",
    "            category = filename  # Dictionary name is each image name\n",
    "            path = folder + \"/\" + filename\n",
    "            img = cv.imread(path, 0)  # Read in grayscale\n",
    "            images[category] = img\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class orb_features:\n",
    "    \"\"\"\n",
    "    ORB detector\n",
    "    \n",
    "    Attributes:\n",
    "        param: Parameters of the ORB detector.\n",
    "        detector: Detector object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.param = dict(nfeatures=50, scaleFactor=1.2, nlevels=4,\n",
    "                          edgeThreshold=31, firstLevel=0, WTA_K=2,\n",
    "                          scoreType=cv.ORB_HARRIS_SCORE,\n",
    "                          patchSize=31, fastThreshold=20)\n",
    "        self.detector = cv.ORB_create(**self.param)\n",
    "\n",
    "    def detectFromDict(self, images_dict: dict) -> [list, dict]:\n",
    "        \"\"\"\n",
    "        Get ORB features from a dictionary of images and save\n",
    "        them in a new dictionary.\n",
    "        \"\"\"\n",
    "        orb = self.detector\n",
    "        descriptor_list = []  # a list of all features\n",
    "        l = [] # a list for the features of each category\n",
    "        descriptor_bycat = {}  # a dictionary of features by categories\n",
    "\n",
    "        for labels, img_loc in images_dict.items():\n",
    "            print(type(img_loc))\n",
    "            if isinstance(img_loc, list):\n",
    "                # if is a list then it is the dictionary with the classified images\n",
    "                for n in range(len(img_loc)):\n",
    "                    nimg_loc = img_loc[n]\n",
    "                    nimg = cv.imread(nimg_loc, 0)\n",
    "                    kp, des = orb.detectAndCompute(nimg, None)\n",
    "                    descriptor_list.extend(des)\n",
    "                    l.extend(des)\n",
    "                descriptor_bycat[labels] = l\n",
    "                l = []\n",
    "            else: \n",
    "                img = cv.imread(img_loc, 0)\n",
    "                kp, des = orb.detectAndCompute(img, None)\n",
    "                descriptor_list.extend(des)\n",
    "                descriptor_bycat[labels] = des\n",
    "\n",
    "        return descriptor_list, descriptor_bycat\n",
    "\n",
    "    def normalizeAllFeatures(self, trainbycat: dict, testbycat: dict): \n",
    "        allfeatureslist = []\n",
    "        traininfo = {}\n",
    "        testinfo = {}\n",
    "        trainlist = []\n",
    "        testlist = []\n",
    "        trainbycat_n = {}\n",
    "        testbycat_n = {}\n",
    "\n",
    "        # Get a list of all features\n",
    "        for cat, features in trainbycat.items():\n",
    "            traininfo[cat] = len(features)\n",
    "            allfeatureslist.extend(features)\n",
    "        for cat, features in testbycat.items():\n",
    "            testinfo[cat] = len(features)\n",
    "            allfeatureslist.extend(features)\n",
    "\n",
    "        # Normalize it\n",
    "        allfeatureslist = preprocessing.normalize(allfeatureslist, norm='l2')\n",
    "\n",
    "        # Get the lists of normalized values for train and test\n",
    "        ntrain = sum(traininfo.values()) \n",
    "        ntest = sum(testinfo.values())\n",
    "        trainlist = allfeatureslist[0:ntrain]\n",
    "        testlist = allfeatureslist[ntrain:ntrain+ntest]\n",
    "\n",
    "        # Organize normalized features by class for train and test\n",
    "        for cat, nfeatures in traininfo.items():\n",
    "            trainbycat_n[cat] = allfeatureslist[0:nfeatures]\n",
    "            allfeatureslist = allfeatureslist[nfeatures+1:-1]\n",
    "        for cat, nfeatures in testinfo.items():\n",
    "            testbycat_n[cat] = allfeatureslist[0:nfeatures]\n",
    "            allfeatureslist = allfeatureslist[nfeatures+1:-1]\n",
    "\n",
    "        return trainlist, trainbycat_n, testlist, testbycat_n\n",
    "\n",
    "\n",
    "class bow():\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Attributes:\n",
    "            t: numerical threshold for classifying.\n",
    "            vw_centers: list of the cluster centers for the visual words.\n",
    "        \"\"\"\n",
    "        self.t = 1 \n",
    "        self.vw_centers = []\n",
    "        self.nclusters = 150\n",
    "    \n",
    "    def setThreshold(self, threshold):\n",
    "        \"\"\"\n",
    "        Sets a new classifying threshold.\n",
    "        \"\"\"\n",
    "        self.t = threshold\n",
    "        \n",
    "    def kmeans(self, descriptor_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A k-means clustering algorithm.\n",
    "        \n",
    "        Inputs:\n",
    "            descriptor_list: Descriptors list\n",
    "                (unordered nfeatures x 32 matrix).\n",
    "        Returns: A matrix [nclusters, 32] that holds \n",
    "            central points of the clusters.\n",
    "        \"\"\"\n",
    "        cluster = KMeans(n_clusters=self.nclusters, n_init=10)\n",
    "        cluster.fit(descriptor_list)\n",
    "        self.vw_centers = cluster.cluster_centers_ \n",
    "\n",
    "        return cluster.cluster_centers_\n",
    "\n",
    "    def compute_histogram(self, featuresbycat: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the histogram for every category.\n",
    "        Inputs:\n",
    "            featuresbycat: Dictionary that contains features \n",
    "                of each category.\n",
    "            centers: A matrix [nclusters, 32] that holds \n",
    "                central points of the clusters.\n",
    "        Returns: A dictionary with the histograms of\n",
    "            each category.\n",
    "        \"\"\"\n",
    "        histogrambycat = {}  # dictionary of histograms\n",
    "        centers = self.vw_centers\n",
    "        histogram = np.zeros([1, self.nclusters])\n",
    "        for cat, features in featuresbycat.items():\n",
    "            for n in range(len(features)):\n",
    "                word = self.matchWord(features[n])\n",
    "                histogram[0, word] += 1\n",
    "            m = np.amax(histogram)\n",
    "            histogram = histogram/m\n",
    "            histogrambycat[cat] = histogram\n",
    "            histogram = np.zeros([1, self.nclusters])\n",
    "\n",
    "        return histogrambycat\n",
    "\n",
    "    def matchWord(self, feature) -> int:\n",
    "        \"\"\"\n",
    "        Finds the matching word searching for the minimum \n",
    "        euclidean distance.\n",
    "        Inputs:\n",
    "            feature: A single feature (descriptor).\n",
    "            centers: A matrix [nclusters, 32] that holds central \n",
    "                points of the clusters.\n",
    "        Returns: \n",
    "            Index of the matched word.\n",
    "        \"\"\"\n",
    "        centers = self.vw_centers\n",
    "        # distance.cdist(coords, coords, 'euclidean')\n",
    "        for n in range(centers.shape[0]):\n",
    "            dist = np.linalg.norm(feature - centers[n, :])  # L2-norm\n",
    "            if n == 0:\n",
    "                mindist = dist\n",
    "                minind = n\n",
    "            elif dist < mindist:\n",
    "                mindist = dist\n",
    "                minind = n\n",
    "\n",
    "        return minind\n",
    "\n",
    "    def matchCategory(self, trainh: dict, testh: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Finds the matching category for every image in test dataset using 1NN\n",
    "        Nearest Neightbour with k=1.\n",
    "        Inputs:\n",
    "            trainh: Dictionary of histograms by class, from training.\n",
    "            testh: Dictionary of histograms by image, from test.\n",
    "        Returns: \n",
    "            Dictionary of the test images classified.\n",
    "        \"\"\"\n",
    "        img_classified = {}\n",
    "        classification = np.nan\n",
    "        # distance.cdist(coords, coords, 'euclidean')\n",
    "        #  aqui habria que calcular la distancia y meter\n",
    "        #  el threshold de algun modo\n",
    "        for imgname, histtest in testh.items():\n",
    "            for cat, hist in trainh.items():\n",
    "                dist = np.linalg.norm(histtest - hist)  # L2-norm\n",
    "                if dist < self.t:\n",
    "                    classification = cat\n",
    "            img_classified[imgname] = classification\n",
    "            classification = np.nan\n",
    "        \n",
    "        similMat = self.createSimilarityMat(img_classified)\n",
    "\n",
    "        return similMat\n",
    "    \n",
    "    def createSimilarityMat(self, classifications: dict):\n",
    "        img_names = list(classifications.keys())\n",
    "        img_cats = list(classifications.values())\n",
    "        size = len(img_names)\n",
    "        similarityMat = np.zeros([size, size])\n",
    "        for ii in range(size):\n",
    "            img_name = img_names[ii]\n",
    "            cat = classifications[img_name]\n",
    "            for jj in range(size):\n",
    "                c = img_cats[jj]\n",
    "                if cat == c and ii != jj:\n",
    "                    similarityMat[ii,jj] = 1\n",
    "        return similarityMat\n",
    "\n",
    "    def findThreshold(self, trainh: dict, testh: dict):\n",
    "        \"\"\"\n",
    "        Finds ans sets a new threshold based on the distances \n",
    "        of the test and training histogram sets.\n",
    "        \"\"\"\n",
    "        dist_list = np.array([])\n",
    "        for imgname, histtest in testh.items():\n",
    "            for cat, hist in trainh.items():\n",
    "                dist = np.linalg.norm(histtest - hist)  # L2-norm\n",
    "                dist_list = np.append(dist_list, dist)\n",
    "        distmin = np.min(dist_list)\n",
    "        distmean = np.mean(dist_list)\n",
    "        t = distmean - (distmean-distmin)/2\n",
    "        self.setThreshold(t)\n",
    "\n",
    "        \n",
    "def main(t):\n",
    "    train_folder = \"dataset/W17\"  # Location of the dataset.\n",
    "    test_folder = \"dataset/W17\"  \n",
    "    \n",
    "    # Get the training dataset\n",
    "    train_dict = getImgpaths(train_folder)\n",
    "    grp_dict = get_grps(train_folder)\n",
    "    loc_bygrp = setGroups(train_dict, grp_dict)\n",
    "    # Get the test dataset\n",
    "    test_dict = getImgpaths(train_folder)\n",
    "\n",
    "    # # Obtain images by name and by class from dataset\n",
    "    # train_dset_bycat = get_images(\"ddataset/train\")\n",
    "    # # train_dset_byname = get_images(\"ddataset/train\")\n",
    "    # test_dset_byname = get_images(\"ddataset/test\")  # take test images\n",
    "\n",
    "    # Gets all the features from the training images\n",
    "    orb = orb_features()\n",
    "    train_features, train_featuresbycat = orb.detectFromDict(loc_bygrp)\n",
    "    # Get all the features from the test images\n",
    "    test_features, test_featuresbycat = orb.detectFromDict(test_dict)\n",
    "    # Normalize them\n",
    "    train_features, train_featuresbycat, test_features, test_featuresbycat = \\\n",
    "    orb.normalizeAllFeatures(train_featuresbycat, test_featuresbycat)\n",
    "\n",
    "    bow_obj = bow()\n",
    "    # Clusters the features using kmeans\n",
    "    bow_obj.kmeans(train_features)\n",
    "    # Compute histograms for every category in train dataset\n",
    "    train_histogrambycat = bow_obj.compute_histogram(train_featuresbycat)\n",
    "\n",
    "    # Compute histrograms for every image in test dataset\n",
    "    test_histogrambycat = bow_obj.compute_histogram(test_featuresbycat)\n",
    "\n",
    "    # Find new threshold\n",
    "    #bow_obj.findThreshold(train_histogrambycat, test_histogrambycat)\n",
    "    bow_obj.setThreshold(t)\n",
    "    # Classify the test images into the trainned categories\n",
    "    classification_results = bow_obj.matchCategory(\n",
    "        train_histogrambycat, test_histogrambycat)\n",
    "    print(\"Classification results: \")\n",
    "    print(classification_results)\n",
    "    \n",
    "    # Evaluation\n",
    "    [Precision, Recall, F1] = PR(gt_labels,ts_labels)\n",
    "    \n",
    "    return Precision, Recall, F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PR(gt_labels,ts_labels):\n",
    "\n",
    "#\" Evaluate Classification \"\n",
    "# Input: Similarity matrix of classification and gound truth\n",
    "\n",
    "    [n,m] = gt_labels.shape\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range (0,n):\n",
    "        for j in range (0,m):\n",
    "            if ts_labels[i][j] == True:\n",
    "                if gt_labels[i][j] == True:\n",
    "                    TP+=1\n",
    "                elif gt_labels[i][j] == False:\n",
    "                    FP+=1\n",
    "            elif ts_labels[i][j] == False:\n",
    "                if gt_labels[i][j] == False:\n",
    "                    TN+=1\n",
    "                elif gt_labels[i][j] == True:\n",
    "                    FN+=1\n",
    "            \n",
    "            \n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1 = 2*(Precision*Recall)/(Precision+Recall)\n",
    "\n",
    "    print(Precision)\n",
    "    print(Recall)\n",
    "    print(F1)\n",
    "    \n",
    "    return Precision, Recall, F1\n",
    "\n",
    "def aveP(Precision, Recall):\n",
    "# Precision and Recall should be a array (by varying threshold)\n",
    "    n = len(Precision)\n",
    "    AP = 0\n",
    "    for i in range(0,n):\n",
    "        Recall = B[i]\n",
    "        Precision = A[i]\n",
    "        if i !=0:\n",
    "            AP+=(Recall-pre)*Precision\n",
    "        else:\n",
    "            AP+=(Recall-0)*Precision\n",
    "        pre = Recall\n",
    "        \n",
    "    return AP\n",
    "\n",
    "def quaternion(qi):\n",
    "# Compute delta phi from quaterions\n",
    "# Input: pose-liked quaternion\n",
    "    \n",
    "    [n,m] = qi.shape\n",
    "    phi = np.zeros((n,n))\n",
    "    for i in range (0,n):\n",
    "        for j in range (0,n):\n",
    "            InProd = 0\n",
    "            if i == j:\n",
    "                phi[i][j] = 0 ;\n",
    "            else:\n",
    "                for k in range (3,7):\n",
    "                    InProd += qi[i][k]*qi[j][k]\n",
    "                \n",
    "                distance = 1 - InProd**2\n",
    "                phi[i][j] = math.acos(-2*distance+1)*180/math.pi  \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4491",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-057bd324c868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Set different classification threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0marrayP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0marrayR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-09f653421673>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImgpaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgrp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloc_bygrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetGroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImgpaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bd5ddeb0726a>\u001b[0m in \u001b[0;36msetGroups\u001b[0;34m(loc_dict, grp_dict)\u001b[0m\n\u001b[1;32m    167\u001b[0m       \u001b[0mloc_bygrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m           \u001b[0mloc_bygrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m       \u001b[0mloc_bygrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4491"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create empty array\n",
    "    arrayP = []\n",
    "    arrayR = []\n",
    "    arrayF = []\n",
    "    \n",
    "    # Set different classification threshold\n",
    "    for i in range(0,11):\n",
    "        [Precision, Recall, F1] = main(i)\n",
    "        arrayP.append(Precision)\n",
    "        arrayR.append(Recall)\n",
    "        arrayF.append(F1)\n",
    "    \n",
    "    # Depict PR curve\n",
    "    plt.figure(1)\n",
    "    plt.plot(arrayR,arrayP)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average precision\n",
    "    AP = aveP(arrayP, arrayR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For threshold that gives the highest F1 score: t0\n",
    "t0 = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depict the relationship between F1 and angle difference \n",
    "[Precision, Recall, F1] = main(t0)\n",
    "    #ts_labels is needed\n",
    "    \n",
    "# Process information of camera angle\n",
    "Q = quaternion(poses)\n",
    "B = np.trunc(Q)  # Covert a float array to an integer array\n",
    "\n",
    "minq = int(np.amin(B)) # To obtain the range of angle difference\n",
    "maxq = int(np.amax(B))\n",
    "[n,m] = B.shape # To obtain the values of colomn and row for the loop\n",
    "\n",
    "A = maxq+1 # Transfer float value to integer\n",
    "qdiff = np.zeros((A,3)) # Create an array to record the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for i in range(0,56): # Loop with angle\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    print(i)\n",
    "    for j in range(0,n):\n",
    "        for k in range(0,m):\n",
    "            if int(B[j][k]) == i and j != k: # First find out the elements with angle = i    \n",
    "                if gt_labels[j][k] == ts_labels[j][k]:\n",
    "                    if gt_labels[j][k] == True:\n",
    "                        tp+=1\n",
    "                else:\n",
    "                    if ts_labels[j][k] == True:\n",
    "                        fp+=1 # gt = false, ts=true\n",
    "                    else:\n",
    "                        fn+=1 # gt = true, ts = false\n",
    "                            \n",
    "    qdiff[i][0] = tp\n",
    "    qdiff[i][1] = fn\n",
    "    qdiff[i][2] = fp\n",
    "    \n",
    "print(qdiff)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate P, R and F1\n",
    "qdiff2 = np.zeros(56) # The array with angle versus F1 score\n",
    "for i in range(minq,A):\n",
    "    tp = qdiff[i][1]\n",
    "    fn = qdiff[i][2]\n",
    "    fp = qdiff[i][3]\n",
    "    f1 = 0\n",
    "    \n",
    "    if tp!=0 or fp!=0 or fn!=0: # P and R will not be nan if one of these (tp,fp,fn) is not zero\n",
    "        p = tp/(tp+fp)\n",
    "        r = tp/(tp+fn)\n",
    "        if p!=0 and r!=0:\n",
    "            f1 = 2*p*r/(p+r)\n",
    "        \n",
    "    qdiff2[i] = f1\n",
    "    \n",
    "print(qdiff2) # The array with angle versus F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,56)\n",
    "plt.figure(1)\n",
    "plt.plot(x,qdiff2)\n",
    "plt.xlabel(\"delta phi\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
